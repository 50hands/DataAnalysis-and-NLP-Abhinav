<h1>Data Analysis and NLP</h1>
<body>
  <p>
    <img alt="GitHub last commit" src="https://img.shields.io/github/last-commit/50hands/DataAnalysis-and-NLP-Abhinav"> 
    <img alt="GitHub repo size" src="https://img.shields.io/github/repo-size/50hands/DataAnalysis-and-NLP-Abhinav"></p>
  <p>
    <a href="https://www.python.org/"><img src="https://img.shields.io/badge/Made%20with-Python-1f425f.svg" alt="made-with-python"></a> 
    <a href="https://public.tableau.com/s/"><img src="https://img.shields.io/badge/Made with-Tableau-pink.svg" alt="Generic badge"></a>
  </p>
  <p style="font-family: 'Lora', serif;">All the work done, code written, files created and visualizations made by me while interning at 50Hands.</p>

  <h3><b>For Emotion Recognition:</b></h3>
  <h5><b>ML & NLP</b></h5>
  <p><a href="https://drive.google.com/drive/u/0/folders/1MtAnQD1dNga5HMzcsg5VSSvkwnEoksBT"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Collab"></a></p>
  <p>A program was made, using Python, that detects emotion based on textual and facial features.</p>
  <p>For textual emotion recognition, LSTM was used to train the model.</p>
  <p>For facial emotion recognition, a CNN network was used to train the model.</p>
  <p>The models were trained using Colab since it offers a GPU, hence faster computations.</p>
  
  <h3><b>For the 2020 US Elections:</b></h3>
  <h5><b>Planning, Scraping & Visualization</b></h5>
  <p><a href="https://drive.google.com/drive/u/0/folders/1GgrZ5KRxKnZ8ciTZyXnJf7V6xogkKPyG"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Collab"></a></p>
  <p>Using Tableau, visualizations were made to show important trends and statistics in relation to the 2020 US elections.</p>
  <p>Some of the trends shown are:</p>
  <ul>
    <li>Voting with respect to geography i.e., state and counties.</li>
    <li>Voting with respect to populations in all the states and counties.</li>
    <li>A state's Democratic or Republican preference with voter demographic population as a factor.</li>
    <li>A region's Democratic or Republican preference with median income and poverty level as a factor, for both, states and counties.</li>
  </ul>
  
  <h3><b>For Canada, India and USA:</b></h3>
  <h5><b>Visualization</b></h5>
  <p>Using Tableau, visualizations were made to show important Coronavirus parameters.</p>
  
  <h3><b>For Google Mobility:</b></h3>
  <h5><b>Scraping & Visualization</b></h5>
  <p>Visualization made to show the mobility data for USA at state and county level.</p>
  <p>A python script written to scrape the mobility data for USA and Canada in a orderly manner that helps in the creation of various visualizations.</p>
  <p>The main script can be found in the Mobility Data folder named Index.py</p>
  
  <h3>For Global Data:</h3>
  <h5><b>Scraping & Visualization</b></h5>
  <p>A script was written to extract global Coronavirus data from John Hopkin's University (from late January), create a data-frame and insert it into the database.</p>
  <p>The data that is inserted into the database is then used to create a Global Covid-19 Tracker, which shows the progression of cases, change in various parameters, country-wise statistics and the relation between those parameters with constants like GDP, poverty rate et cetera.</p>
  <p>The main script can be found in the 'World' folder named Index.py.</p>
  
  <h3>For Twitter Sentiment Analysis:</h3>
  <h5><b>Visualization</b></h5>
  <p>For Canada, India and USA, a visualization was made for Twitter Sentiment Analysis.</p>
  
  <h3>For Reddit Sentiment Analysis:</h3>
  <h5><b>NLP & Visualization</b></h5>
  <p>For various countries, posts were extracted from their designated Covid-19 subreddits and sentiment analysis was performed.</p>
  <p>A visualization was made showing these sentiment statistics.</p>
  <p>For Canada, India and USA, wordclouds and bigram-clouds were created using the same post extraction process as before.</p>
  <p>For this too visualizations were created.</p>
  
  <h3>For Quotes Generation:</h3>
  <h5><b>NLP</b></h5>
  <p><a href="https://colab.research.google.com/drive/1zGTJxbZr6OMBcUlpwD-f-AXf-GNlaT1C"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Collab"></a></p>
  <p>GPT-2 was trained using a csv of quotes, in the hope that the trained model would generate similar quotes, that made sense.</p>
  <p>Most of the code was already present and can be found in GPT-2's documentation.</p>
  <p>A few modifications were made to put the generated quotes into a CSV file, so that a quote could be extracted when the API for that is called in negligible time.</p>
  <p>For reference: <a href='https://github.com/openai/gpt-2'>https://github.com/openai/gpt-2</a></p>
  
  <br>
  <p><a href="https://GitHub.com/abhinavyesss/"><img src="http://ForTheBadge.com/images/badges/built-by-developers.svg" alt="ForTheBadge built-by-developers"></a> <a href="https://GitHub.com/abhinavyesss/"><img src="http://ForTheBadge.com/images/badges/built-with-love.svg" alt="ForTheBadge built-with-love"></a> <a href="https://GitHub.com/abhinavyesss/"><img src="http://ForTheBadge.com/images/badges/built-with-science.svg" alt="ForTheBadge built-with-science"></a></p>
  <br>
  <p><a href="http://ForTheBadge.com"><img src="http://ForTheBadge.com/images/badges/winter-is-coming.svg" alt="ForTheBadge winter-is-coming"></a></p>
</body>
